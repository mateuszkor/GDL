{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "55e7004b",
      "metadata": {},
      "source": [
        "# Experiment: GNN Failure Risk Audit\n",
        "\n",
        "Objective:\n",
        "- Build a multi signal audit score using homophily, feature label alignment, and a spectral summary.\n",
        "- Test whether the audit score predicts GNN accuracy better than homophily alone.\n",
        "- Stress test the audit across multiple synthetic graph families and validate on real datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d86149",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab install (minimal, conditional)\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        import torch_geometric  # noqa: F401\n",
        "        print('PyG already installed')\n",
        "    except Exception:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch-geometric', 'scipy', 'networkx'])\n",
        "else:\n",
        "    print('Local run detected. Ensure torch-geometric + scipy + networkx are installed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3c96f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports + seed + device\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import from_networkx, add_self_loops\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd1240f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic data (SBM, LFR, Degree-skew, 500 nodes)\n",
        "\n",
        "def make_sbm(num_nodes=500, p_in=0.1, p_out=0.01, num_classes=2, feat_dim=16, feat_strength=0.1, sizes=None, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    if sizes is None:\n",
        "        sizes = [num_nodes // num_classes for _ in range(num_classes)]\n",
        "        sizes[0] += num_nodes - sum(sizes)\n",
        "    else:\n",
        "        num_nodes = sum(sizes)\n",
        "    probs = [[p_in if i == j else p_out for j in range(num_classes)] for i in range(num_classes)]\n",
        "\n",
        "    G = nx.stochastic_block_model(sizes, probs, seed=seed)\n",
        "    labels = []\n",
        "    for c, size in enumerate(sizes):\n",
        "        labels.extend([c] * size)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    features = rng.normal(size=(num_nodes, feat_dim)).astype(np.float32)\n",
        "    features += feat_strength * rng.normal(size=(num_nodes, feat_dim)).astype(np.float32) * labels[:, None]\n",
        "\n",
        "    data = from_networkx(G)\n",
        "    data.x = torch.tensor(features, dtype=torch.float)\n",
        "    data.y = torch.tensor(labels, dtype=torch.long)\n",
        "    return data\n",
        "\n",
        "\n",
        "def smooth_labels(G, labels, steps=3, prob=0.7, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    labels = labels.copy()\n",
        "    for _ in range(steps):\n",
        "        for node in G.nodes():\n",
        "            neigh = list(G.neighbors(node))\n",
        "            if not neigh:\n",
        "                continue\n",
        "            counts = np.bincount(labels[neigh], minlength=2)\n",
        "            majority = int(counts.argmax())\n",
        "            if rng.random() < prob:\n",
        "                labels[node] = majority\n",
        "    return labels\n",
        "\n",
        "\n",
        "def ensure_two_classes(labels):\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        labels[0] = 1 - labels[0]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def make_powerlaw(num_nodes=500, m=3, p=0.05, num_classes=2, feat_dim=16, feat_strength=0.1, smooth_steps=0, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    G = nx.powerlaw_cluster_graph(num_nodes, m=m, p=p, seed=seed)\n",
        "    labels = rng.integers(0, num_classes, size=num_nodes)\n",
        "    if smooth_steps > 0:\n",
        "        labels = smooth_labels(G, labels, steps=smooth_steps, seed=seed)\n",
        "    labels = ensure_two_classes(labels)\n",
        "\n",
        "    features = rng.normal(size=(num_nodes, feat_dim)).astype(np.float32)\n",
        "    features += feat_strength * rng.normal(size=(num_nodes, feat_dim)).astype(np.float32) * labels[:, None]\n",
        "\n",
        "    data = from_networkx(G)\n",
        "    data.x = torch.tensor(features, dtype=torch.float)\n",
        "    data.y = torch.tensor(labels, dtype=torch.long)\n",
        "    return data\n",
        "\n",
        "\n",
        "def make_lfr(num_nodes=500, mu=0.3, tau1=3.0, tau2=1.5, avg_deg=6, min_comm=20, max_comm=50,\n",
        "             num_classes=2, feat_dim=16, feat_strength=0.1, seed=0, max_tries=8):\n",
        "    G = None\n",
        "    last_err = None\n",
        "    for k in range(max_tries):\n",
        "        try:\n",
        "            G = nx.LFR_benchmark_graph(\n",
        "                num_nodes, tau1, tau2, mu,\n",
        "                average_degree=avg_deg,\n",
        "                min_community=min_comm,\n",
        "                max_community=max_comm,\n",
        "                seed=seed + k,\n",
        "            )\n",
        "            G = nx.Graph(G)  # make simple\n",
        "            if G.number_of_nodes() == num_nodes:\n",
        "                break\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            G = None\n",
        "    if G is None:\n",
        "        raise RuntimeError(f\"LFR generation failed: {last_err}\")\n",
        "\n",
        "    comm_attr = nx.get_node_attributes(G, 'community')\n",
        "    communities = {}\n",
        "    labels = np.zeros(num_nodes, dtype=np.int64)\n",
        "    for node in G.nodes():\n",
        "        comm = comm_attr.get(node, {node})\n",
        "        key = tuple(sorted(comm))\n",
        "        if key not in communities:\n",
        "            communities[key] = len(communities)\n",
        "        comm_id = communities[key]\n",
        "        labels[node] = comm_id % num_classes\n",
        "    labels = ensure_two_classes(labels)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    features = rng.normal(size=(num_nodes, feat_dim)).astype(np.float32)\n",
        "    features += feat_strength * rng.normal(size=(num_nodes, feat_dim)).astype(np.float32) * labels[:, None]\n",
        "\n",
        "    data = from_networkx(G)\n",
        "    data.x = torch.tensor(features, dtype=torch.float)\n",
        "    data.y = torch.tensor(labels, dtype=torch.long)\n",
        "    return data\n",
        "\n",
        "\n",
        "def homophily_ratio(edge_index, labels):\n",
        "    src, dst = edge_index\n",
        "    same = (labels[src] == labels[dst]).float()\n",
        "    return same.mean().item() if same.numel() > 0 else 0.0\n",
        "\n",
        "\n",
        "def feature_label_alignment(x, y, num_classes=2):\n",
        "    x = x.float()\n",
        "    y = y.long()\n",
        "    means = []\n",
        "    for c in range(num_classes):\n",
        "        if (y == c).sum() == 0:\n",
        "            means.append(torch.zeros(x.shape[1]))\n",
        "        else:\n",
        "            means.append(x[y == c].mean(0))\n",
        "    means = torch.stack(means, dim=0)\n",
        "    x_norm = F.normalize(x, p=2, dim=1)\n",
        "    means_norm = F.normalize(means, p=2, dim=1)\n",
        "    sims = (x_norm * means_norm[y]).sum(dim=1)\n",
        "    return float(((sims + 1.0) / 2.0).mean().item())\n",
        "\n",
        "\n",
        "def spectral_summaries(edge_index, num_nodes):\n",
        "    A = torch.zeros((num_nodes, num_nodes), dtype=torch.float32)\n",
        "    A[edge_index[0], edge_index[1]] = 1.0\n",
        "    A = torch.maximum(A, A.T)\n",
        "    deg = A.sum(dim=1)\n",
        "    deg_inv_sqrt = torch.where(deg > 0, deg.pow(-0.5), torch.zeros_like(deg))\n",
        "    D_inv_sqrt = torch.diag(deg_inv_sqrt)\n",
        "    L = torch.eye(num_nodes) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "    eigvals = torch.linalg.eigvalsh(L).cpu().numpy()\n",
        "    lambda_max = float(eigvals[-1])\n",
        "    lambda2 = float(eigvals[1]) if len(eigvals) > 1 else 0.0\n",
        "    return lambda_max, lambda2\n",
        "\n",
        "\n",
        "def multi_risk_score(homophily, alignment, lambda_max):\n",
        "    risk_h = 1.0 - homophily\n",
        "    risk_a = 1.0 - alignment\n",
        "    risk_s = lambda_max / 2.0\n",
        "    return (risk_h + risk_a + risk_s) / 3.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b3d813",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models + training helpers\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "        )\n",
        "    def forward(self, x, edge_index=None):\n",
        "        return self.net(x)\n",
        "class GPRGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, K=10, alpha=0.1):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(hidden_dim, out_dim)\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "        # PPR style init\n",
        "        gamma = [(alpha * (1 - alpha) ** k) for k in range(K + 1)]\n",
        "        self.gamma = nn.Parameter(torch.tensor(gamma, dtype=torch.float))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        num_nodes = x.size(0)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
        "        row, col = edge_index\n",
        "        deg = torch.bincount(row, minlength=num_nodes).float()\n",
        "        deg_inv_sqrt = torch.pow(deg, -0.5)\n",
        "        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
        "        values = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        A = torch.sparse_coo_tensor(edge_index, values, (num_nodes, num_nodes))\n",
        "\n",
        "        out = self.gamma[0] * x\n",
        "        xk = x\n",
        "        for k in range(1, self.K + 1):\n",
        "            xk = torch.sparse.mm(A, xk)\n",
        "            out = out + self.gamma[k] * xk\n",
        "        return out\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "def split_indices(n, train_ratio=0.6, val_ratio=0.2, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(n)\n",
        "    n_train = int(train_ratio * n)\n",
        "    n_val = int(val_ratio * n)\n",
        "    train_idx = torch.tensor(idx[:n_train], dtype=torch.long)\n",
        "    val_idx = torch.tensor(idx[n_train:n_train + n_val], dtype=torch.long)\n",
        "    test_idx = torch.tensor(idx[n_train + n_val:], dtype=torch.long)\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "def train_epoch(model, data, train_idx, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.cross_entropy(out[train_idx], data.y[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def eval_acc(model, data, idx):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=-1)\n",
        "        acc = (pred[idx] == data.y[idx]).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(in_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, out_dim, heads=1, concat=False)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e90b62f",
      "metadata": {},
      "source": [
        "## What this experiment does\n",
        "- Builds a stress suite of synthetic graph families, including SBM with balanced and imbalanced communities, LFR style graphs with varying mixing, and degree skew graphs.\n",
        "- Trains five models, including GPRGNN on each graph and records accuracy, generalization gap, and structural signals.\n",
        "- Computes a multi signal audit score that combines homophily, a feature label alignment proxy, and a spectral summary.\n",
        "- Tests whether the audit score predicts accuracy better than homophily alone across families.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bcd4128",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run stress suite with multiple seeds\n",
        "\n",
        "families = [\n",
        "    {\n",
        "        'name': 'SBM-balanced',\n",
        "        'type': 'sbm',\n",
        "        'sizes': None,\n",
        "        'settings': [(0.12, 0.01), (0.10, 0.01), (0.07, 0.03), (0.03, 0.05)],\n",
        "    },\n",
        "    {\n",
        "        'name': 'SBM-imbalanced',\n",
        "        'type': 'sbm',\n",
        "        'sizes': [300, 200],\n",
        "        'settings': [(0.12, 0.01), (0.10, 0.01), (0.07, 0.03), (0.03, 0.05)],\n",
        "    },\n",
        "    {\n",
        "        'name': 'LFR',\n",
        "        'type': 'lfr',\n",
        "        'settings': [0.1, 0.3, 0.5],  # mu\n",
        "    },\n",
        "    {\n",
        "        'name': 'DegreeSkew',\n",
        "        'type': 'powerlaw',\n",
        "        'settings': [(2, 0), (2, 4), (4, 0), (4, 4)],  # (m, smooth_steps)\n",
        "    },\n",
        "]\n",
        "\n",
        "num_seeds = 3\n",
        "rows = []\n",
        "\n",
        "for fam in families:\n",
        "    for setting in fam['settings']:\n",
        "        for s in range(num_seeds):\n",
        "            seed = SEED + s\n",
        "\n",
        "            if fam['type'] == 'sbm':\n",
        "                p_in, p_out = setting\n",
        "                data = make_sbm(p_in=p_in, p_out=p_out, sizes=fam['sizes'], seed=seed)\n",
        "                setting_id = f\"p_in={p_in:.2f},p_out={p_out:.2f}\"\n",
        "            elif fam['type'] == 'lfr':\n",
        "                mu = setting\n",
        "                data = make_lfr(mu=mu, seed=seed)\n",
        "                setting_id = f\"mu={mu:.2f}\"\n",
        "            else:\n",
        "                m, smooth_steps = setting\n",
        "                data = make_powerlaw(m=m, smooth_steps=smooth_steps, seed=seed)\n",
        "                setting_id = f\"m={m},smooth={smooth_steps}\"\n",
        "\n",
        "            # structural signals\n",
        "            h_ratio = homophily_ratio(data.edge_index, data.y)\n",
        "            alignment = feature_label_alignment(data.x, data.y)\n",
        "            lambda_max, lambda2 = spectral_summaries(data.edge_index, data.num_nodes)\n",
        "            risk_multi = multi_risk_score(h_ratio, alignment, lambda_max)\n",
        "\n",
        "            data = data.to(DEVICE)\n",
        "            train_idx, val_idx, test_idx = split_indices(data.num_nodes, seed=seed)\n",
        "\n",
        "            for model_name, model in [\n",
        "            ('MLP', MLP(data.num_features, 32, 2)),\n",
        "            ('GCN', GCN(data.num_features, 32, 2)),\n",
        "            ('GraphSAGE', GraphSAGE(data.num_features, 32, 2)),\n",
        "            ('GAT', GAT(data.num_features, 32, 2)),\n",
        "            ('GPRGNN', GPRGNN(data.num_features, 32, 2)),\n",
        "        ]:\n",
        "                model = model.to(DEVICE)\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "                for _ in range(50):\n",
        "                    train_epoch(model, data, train_idx, optimizer)\n",
        "                train_acc = eval_acc(model, data, train_idx)\n",
        "                test_acc = eval_acc(model, data, test_idx)\n",
        "                rows.append({\n",
        "                    'family': fam['name'],\n",
        "                    'setting': setting_id,\n",
        "                    'seed': seed,\n",
        "                    'model': model_name,\n",
        "                    'homophily': h_ratio,\n",
        "                    'alignment': alignment,\n",
        "                    'lambda_max': lambda_max,\n",
        "                    'lambda2': lambda2,\n",
        "                    'risk_multi': risk_multi,\n",
        "                    'train_acc': train_acc,\n",
        "                    'test_acc': test_acc,\n",
        "                    'gen_gap': train_acc - test_acc,\n",
        "                })\n",
        "                print(f\"{fam['name']} {setting_id} seed={seed} {model_name} test_acc={test_acc:.3f}\")\n",
        "\n",
        "print(\"Summary table (raw):\")\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.head())\n",
        "\n",
        "# Aggregate\n",
        "agg = df.groupby(['model', 'family', 'setting']).agg(\n",
        "    homophily=('homophily', 'mean'),\n",
        "    alignment=('alignment', 'mean'),\n",
        "    lambda_max=('lambda_max', 'mean'),\n",
        "    risk_multi=('risk_multi', 'mean'),\n",
        "    test_acc_mean=('test_acc', 'mean'),\n",
        "    test_acc_std=('test_acc', 'std'),\n",
        "    gen_gap_mean=('gen_gap', 'mean'),\n",
        "    gen_gap_std=('gen_gap', 'std'),\n",
        ").reset_index()\n",
        "\n",
        "print(\"Summary table (mean \u00b1 std):\")\n",
        "print(agg)\n",
        "\n",
        "# Plot 1: mean accuracy vs homophily (connect points within each model)\n",
        "plt.figure(figsize=(6, 4))\n",
        "for model, g in agg.groupby('model'):\n",
        "    g = g.sort_values('homophily')\n",
        "    plt.plot(g['homophily'], g['test_acc_mean'], marker='o', label=model, alpha=0.8)\n",
        "plt.xlabel('Homophily ratio')\n",
        "plt.ylabel('Test accuracy (mean)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: generalization gap vs homophily (connect points within each model)\n",
        "plt.figure(figsize=(6, 4))\n",
        "for model, g in agg.groupby('model'):\n",
        "    g = g.sort_values('homophily')\n",
        "    plt.plot(g['homophily'], g['gen_gap_mean'], marker='o', label=model, alpha=0.8)\n",
        "plt.xlabel('Homophily ratio')\n",
        "plt.ylabel('Generalization gap (mean)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: multi-signal risk vs accuracy (connect points within each model)\n",
        "plt.figure(figsize=(6, 4))\n",
        "for model, g in agg.groupby('model'):\n",
        "    g = g.sort_values('risk_multi')\n",
        "    plt.plot(g['risk_multi'], g['test_acc_mean'], marker='o', label=model, alpha=0.8)\n",
        "plt.xlabel('Multi-signal risk score')\n",
        "plt.ylabel('Test accuracy (mean)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare predictive strength: homophily vs multi-signal risk\n",
        "print(\"Correlation with test accuracy (all families):\")\n",
        "for model, g in df.groupby('model'):\n",
        "    corr_h = g['homophily'].corr(g['test_acc'])\n",
        "    corr_r = g['risk_multi'].corr(g['test_acc'])\n",
        "    print(model, f\"homophily={corr_h:.3f}, risk={corr_r:.3f}\")\n",
        "\n",
        "print(\"Correlation with test accuracy (by family):\")\n",
        "for family, g in df.groupby('family'):\n",
        "    corr_h = g['homophily'].corr(g['test_acc'])\n",
        "    corr_r = g['risk_multi'].corr(g['test_acc'])\n",
        "    print(family, f\"homophily={corr_h:.3f}, risk={corr_r:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea8d14c",
      "metadata": {},
      "source": [
        "## Alignment (Feature-Signal) Sweep\n",
        "We now keep graph structure fixed and vary how much label signal is present in node features.\n",
        "This probes the misalignment idea: when features are weakly aligned with labels, GNN performance should drop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767207aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alignment sweep (vary feature strength)\n",
        "feature_strengths = [0.0, 0.1, 0.3, 0.6]\n",
        "\n",
        "rows_align = []\n",
        "\n",
        "# Fix graph structure to a moderately homophilic regime\n",
        "p_in, p_out = (0.10, 0.01)\n",
        "\n",
        "for fs in feature_strengths:\n",
        "    for s in range(num_seeds):\n",
        "        seed = SEED + 100 + s\n",
        "        data = make_sbm(p_in=p_in, p_out=p_out, seed=seed)\n",
        "        data = data.to(DEVICE)\n",
        "\n",
        "        # overwrite features with controllable signal strength\n",
        "        rng = np.random.default_rng(seed)\n",
        "        features = rng.normal(size=(data.num_nodes, data.num_features)).astype(np.float32)\n",
        "        features += fs * rng.normal(size=(data.num_nodes, data.num_features)).astype(np.float32) * data.y.cpu().numpy()[:, None]\n",
        "        data.x = torch.tensor(features, dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        train_idx, val_idx, test_idx = split_indices(data.num_nodes, seed=seed)\n",
        "\n",
        "        for model_name, model in [\n",
        "            ('MLP', MLP(data.num_features, 32, 2)),\n",
        "            ('GCN', GCN(data.num_features, 32, 2)),\n",
        "            ('GraphSAGE', GraphSAGE(data.num_features, 32, 2)),\n",
        "            ('GAT', GAT(data.num_features, 32, 2)),\n",
        "            ('GPRGNN', GPRGNN(data.num_features, 32, 2)),\n",
        "        ]:\n",
        "            model = model.to(DEVICE)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "            for _ in range(50):\n",
        "                train_epoch(model, data, train_idx, optimizer)\n",
        "            test_acc = eval_acc(model, data, test_idx)\n",
        "            rows_align.append({\n",
        "                'feature_strength': fs,\n",
        "                'seed': seed,\n",
        "                'model': model_name,\n",
        "                'test_acc': test_acc,\n",
        "            })\n",
        "\n",
        "align_df = pd.DataFrame(rows_align)\n",
        "\n",
        "agg_align = align_df.groupby(['model', 'feature_strength']).agg(\n",
        "    test_acc_mean=('test_acc', 'mean'),\n",
        "    test_acc_std=('test_acc', 'std'),\n",
        ").reset_index()\n",
        "\n",
        "print(\"Alignment sweep (mean \u00b1 std):\")\n",
        "print(agg_align)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for model, g in agg_align.groupby('model'):\n",
        "    plt.errorbar(g['feature_strength'], g['test_acc_mean'], yerr=g['test_acc_std'], fmt='o', label=model, alpha=0.8)\n",
        "plt.xlabel('Feature signal strength')\n",
        "plt.ylabel('Test accuracy (mean \u00b1 std)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9268af",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bd1b8f84",
      "metadata": {},
      "source": [
        "## Real Datasets \n",
        "\n",
        "This section adds Squirrel as a heterophily benchmark.\n",
        "We now run a small check on Cora, CiteSeer, and Squirrel. This section downloads datasets in Colab and may take a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f941a18",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid, WikipediaNetwork\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "REAL_DATASETS = [\"Cora\", \"CiteSeer\", \"Squirrel\"]\n",
        "num_seeds_real = 5\n",
        "\n",
        "\n",
        "def get_masks(data):\n",
        "    if data.train_mask.dim() == 2:\n",
        "        train_mask = data.train_mask[:, 0]\n",
        "        val_mask = data.val_mask[:, 0]\n",
        "        test_mask = data.test_mask[:, 0]\n",
        "    else:\n",
        "        train_mask = data.train_mask\n",
        "        val_mask = data.val_mask\n",
        "        test_mask = data.test_mask\n",
        "    train_idx = train_mask.nonzero(as_tuple=False).view(-1)\n",
        "    val_idx = val_mask.nonzero(as_tuple=False).view(-1)\n",
        "    test_idx = test_mask.nonzero(as_tuple=False).view(-1)\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name in [\"Cora\", \"CiteSeer\"]:\n",
        "        root = \"/content/planetoid\" if IN_COLAB else \"output/datasets/Planetoid\"\n",
        "        dataset = Planetoid(root=root, name=name)\n",
        "    elif name == \"Squirrel\":\n",
        "        root = \"/content/wikipedia\" if IN_COLAB else \"output/datasets/WikipediaNetwork\"\n",
        "        dataset = WikipediaNetwork(root=root, name=\"squirrel\", geom_gcn_preprocess=True)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {name}\")\n",
        "    data = dataset[0]\n",
        "    data.edge_index = to_undirected(data.edge_index)\n",
        "    return data\n",
        "\n",
        "\n",
        "real_rows = []\n",
        "for name in REAL_DATASETS:\n",
        "    data = load_dataset(name)\n",
        "    data = data.to(DEVICE)\n",
        "\n",
        "    train_idx, val_idx, test_idx = get_masks(data)\n",
        "\n",
        "    h_ratio = homophily_ratio(data.edge_index, data.y)\n",
        "\n",
        "    for s in range(num_seeds_real):\n",
        "        seed = SEED + 1000 + s\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        for model_name, model in [\n",
        "            ('MLP', MLP(data.num_features, 32, int(data.y.max().item()) + 1)),\n",
        "            ('GCN', GCN(data.num_features, 32, int(data.y.max().item()) + 1)),\n",
        "            ('GraphSAGE', GraphSAGE(data.num_features, 32, int(data.y.max().item()) + 1)),\n",
        "            ('GAT', GAT(data.num_features, 32, int(data.y.max().item()) + 1)),\n",
        "            ('GPRGNN', GPRGNN(data.num_features, 32, int(data.y.max().item()) + 1)),\n",
        "        ]:\n",
        "            model = model.to(DEVICE)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "            for _ in range(100):\n",
        "                train_epoch(model, data, train_idx, optimizer)\n",
        "            test_acc = eval_acc(model, data, test_idx)\n",
        "            real_rows.append({\n",
        "                'dataset': name,\n",
        "                'homophily': h_ratio,\n",
        "                'seed': seed,\n",
        "                'model': model_name,\n",
        "                'test_acc': test_acc,\n",
        "            })\n",
        "\n",
        "real_df = pd.DataFrame(real_rows)\n",
        "\n",
        "agg_real = real_df.groupby(['dataset', 'model']).agg(\n",
        "    homophily=('homophily', 'mean'),\n",
        "    test_acc_mean=('test_acc', 'mean'),\n",
        "    test_acc_std=('test_acc', 'std'),\n",
        ").reset_index()\n",
        "\n",
        "print(agg_real)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for model, g in agg_real.groupby('model'):\n",
        "    plt.errorbar(g['homophily'], g['test_acc_mean'], yerr=g['test_acc_std'], fmt='o', label=model, alpha=0.8)\n",
        "plt.xlabel('Homophily ratio')\n",
        "plt.ylabel('Test accuracy (mean \u00b1 std)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}